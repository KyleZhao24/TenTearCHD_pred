{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5S87ajUNhu_y",
      "metadata": {
        "id": "5S87ajUNhu_y"
      },
      "source": [
        "#  Set parameters and initialize aiplatform client library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b2ZGwU4JUMdt2bZ64nJBbYl9",
      "metadata": {
        "executionInfo": {
          "elapsed": 255,
          "status": "ok",
          "timestamp": 1714619717441,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "b2ZGwU4JUMdt2bZ64nJBbYl9",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Set parameters\n",
        "project_id = 'ise543-419203'\n",
        "location = 'us-central1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4ZWY5F_vh8nz",
      "metadata": {
        "executionInfo": {
          "elapsed": 1501,
          "status": "ok",
          "timestamp": 1714619719120,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "4ZWY5F_vh8nz"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "aiplatform.init(project=project_id, location=location)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "t8kXmAvbiDsd",
      "metadata": {
        "executionInfo": {
          "elapsed": 1,
          "status": "ok",
          "timestamp": 1714619719120,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "t8kXmAvbiDsd"
      },
      "outputs": [],
      "source": [
        "# !pip install kfp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cp08C-t9wgTS",
      "metadata": {
        "executionInfo": {
          "elapsed": 1,
          "status": "ok",
          "timestamp": 1714619719120,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "cp08C-t9wgTS"
      },
      "outputs": [],
      "source": [
        "# !pip install gcsfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "MSjZquQZu01r",
      "metadata": {
        "executionInfo": {
          "elapsed": 1,
          "status": "ok",
          "timestamp": 1714619719120,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "MSjZquQZu01r"
      },
      "outputs": [],
      "source": [
        "# !pip install fsspec"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XDEqG72bh6kp",
      "metadata": {
        "id": "XDEqG72bh6kp"
      },
      "source": [
        "# Define components"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wz40TulUiL_j",
      "metadata": {
        "id": "wz40TulUiL_j"
      },
      "source": [
        "# Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "Ur-VKqHriFzk",
      "metadata": {
        "executionInfo": {
          "elapsed": 425,
          "status": "ok",
          "timestamp": 1714619719544,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "Ur-VKqHriFzk"
      },
      "outputs": [],
      "source": [
        "from kfp.dsl import pipeline, component\n",
        "from kfp.dsl import InputPath, OutputPath, Dataset\n",
        "from kfp.dsl import Artifact\n",
        "from kfp.dsl import Output, Input\n",
        "\n",
        "@component(base_image='python:3.11',\n",
        "           packages_to_install=[\"pandas\", \"fsspec\", \"gcsfs\"])\n",
        "def import_data(\n",
        "    df_output: Output[Dataset]\n",
        "):\n",
        "    import pandas as pd\n",
        "\n",
        "    # Specify the correct path to the CSV file in Google Cloud Storage\n",
        "    df_path = 'gs://ise543_final_pj/Final Project Dataset.csv'\n",
        "\n",
        "\n",
        "    # Read data from the CSV file\n",
        "    df = pd.read_csv(df_path)\n",
        "\n",
        "\n",
        "    # Save the DataFrame to the output path provided by the Kubeflow Pipeline component\n",
        "    df.to_csv(df_output.path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5Z4Xm4z4ka69",
      "metadata": {
        "id": "5Z4Xm4z4ka69"
      },
      "source": [
        "## Import test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "GPx3tCwQkdsq",
      "metadata": {
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1714619719544,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "GPx3tCwQkdsq"
      },
      "outputs": [],
      "source": [
        "@component(base_image='python:3.11',\n",
        "           packages_to_install=[\"pandas\", \"fsspec\", \"gcsfs\"])\n",
        "def import_test_data(\n",
        "    df_test_output: Output[Dataset]\n",
        "):\n",
        "\n",
        "    import pandas as pd\n",
        "\n",
        "    #  test df_path\n",
        "    df_test_path = 'gs://ise543_final_pj/Final Project Evaluation Dataset - Student(1).csv'\n",
        "\n",
        "\n",
        "    df_test = pd.read_csv(df_test_path)\n",
        "\n",
        "    df_test.to_csv(df_test_output.path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q-x31DSd3rvz",
      "metadata": {
        "id": "Q-x31DSd3rvz"
      },
      "source": [
        "# Sperate ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "IsJOzt6N3qxl",
      "metadata": {
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1714619719544,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "IsJOzt6N3qxl"
      },
      "outputs": [],
      "source": [
        "@component(base_image='python:3.11',\n",
        "           packages_to_install=[\"pandas\"])\n",
        "def separate_id(\n",
        "    df_input: Input[Dataset],\n",
        "    ID_output: Output[Dataset],\n",
        "    processed_data_output: Output[Dataset]\n",
        "):\n",
        "    import pandas as pd\n",
        "\n",
        "    # Read data\n",
        "    data = pd.read_csv(df_input.path)\n",
        "\n",
        "\n",
        "    # Separate out the ID column and assume the column name is 'ID'\n",
        "    ids = data[['patientID']]\n",
        "    data_without_id = data.drop(columns=['patientID'])\n",
        "\n",
        "\n",
        "\n",
        "    # Save the ID column to the specified output path\n",
        "    ids.to_csv(ID_output.path, index=False)\n",
        "\n",
        "\n",
        "    # Save processed data without ID to another output path\n",
        "    data_without_id.to_csv(processed_data_output.path, index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iSQD3_L70xA8",
      "metadata": {
        "id": "iSQD3_L70xA8"
      },
      "source": [
        "## Sperate ID in test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3snFPHp101L3",
      "metadata": {
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1714619719544,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "3snFPHp101L3"
      },
      "outputs": [],
      "source": [
        "@component(base_image='python:3.11',\n",
        "           packages_to_install=[\"pandas\"])\n",
        "def separate_id_test(\n",
        "    df_test_input: Input[Dataset],\n",
        "    ID_test_output: Output[Dataset],\n",
        "    processed_test_data_output: Output[Dataset]\n",
        "):\n",
        "    import pandas as pd\n",
        "\n",
        "    data_test = pd.read_csv(df_test_input.path)\n",
        "\n",
        "    # Separate out the ID column and assume the column name is 'ID'\n",
        "    ids_test = data_test[['patientID']]\n",
        "    data_without_id_test = data_test.drop(columns=['patientID'])\n",
        "\n",
        "    ids_test.to_csv(ID_test_output.path, index=False)\n",
        "\n",
        "    data_without_id_test.to_csv(processed_test_data_output.path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "OM0DVQ8P2Ccj",
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "ok",
          "timestamp": 1714619719545,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "OM0DVQ8P2Ccj"
      },
      "outputs": [],
      "source": [
        "# @component(base_image='python:3.11',\n",
        "#            packages_to_install=[\"pandas\"])\n",
        "# def drop_target_test(\n",
        "#     processed_test_data_input: Input[Dataset],\n",
        "#     X_test_output: Output[Dataset]\n",
        "#     # y_test_output: Output[Dataset]\n",
        "\n",
        "# ):\n",
        "#     import pandas as pd\n",
        "\n",
        "#     # Load the processed data\n",
        "#     data = pd.read_csv(processed_test_data_input.path)\n",
        "\n",
        "#     # Separate features and target using 'TenYearCHD' as the target column\n",
        "#     X_test = data.drop(columns=['TenYearCHD'])\n",
        "#     # y_test = data['TenYearCHD']\n",
        "\n",
        "#     X_test.to_csv(X_test_output.path, index=False)\n",
        "#     # y_test.to_csv(y_test_output.path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zBkZ2HHj4I_i",
      "metadata": {
        "id": "zBkZ2HHj4I_i"
      },
      "source": [
        "# Split data into train and val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0BbayHbk4N2G",
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "ok",
          "timestamp": 1714619719545,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "0BbayHbk4N2G"
      },
      "outputs": [],
      "source": [
        "@component(base_image='python:3.11',\n",
        "           packages_to_install=[\"pandas\", \"scikit-learn\"])\n",
        "def split_data(\n",
        "    processed_data_input: Input[Dataset],\n",
        "    X_train_output: Output[Dataset],\n",
        "    X_val_output: Output[Dataset],\n",
        "    y_train_output: Output[Dataset],\n",
        "    y_val_output: Output[Dataset],\n",
        "    split_details: Output[Artifact],  # Output details of the split\n",
        "\n",
        "\n",
        "\n",
        "    split_ratio: float = 0.2,\n",
        "    random_seed: int = 42,\n",
        "):\n",
        "    import pandas as pd\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    # Load the processed data\n",
        "    data = pd.read_csv(processed_data_input.path)\n",
        "\n",
        "    # Separate features and target using 'TenYearCHD' as the target column\n",
        "    X = data.drop(columns=['TenYearCHD'])\n",
        "    y = data['TenYearCHD']\n",
        "\n",
        "\n",
        "    # Perform the split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=split_ratio, random_state=random_seed)\n",
        "\n",
        "    # Save the split datasets\n",
        "    X_train.to_csv(X_train_output.path, index=False)\n",
        "    X_val.to_csv(X_val_output.path, index=False)\n",
        "    y_train.to_csv(y_train_output.path, index=False)\n",
        "    y_val.to_csv(y_val_output.path, index=False)\n",
        "\n",
        "    # Optionally save some details about the split\n",
        "    details = f\"Train size: {len(X_train)}, Validation size: {len(X_val)}\"\n",
        "    with open(split_details.path, 'w') as f:\n",
        "        f.write(details)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wCPWwpi84WlR",
      "metadata": {
        "id": "wCPWwpi84WlR"
      },
      "source": [
        "## Feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "NRbar8c84cpI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 5,
          "status": "ok",
          "timestamp": 1714619719545,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "NRbar8c84cpI",
        "outputId": "2318261a-d1ed-4e1c-a1c1-657de6bd8b99"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-4ccef15704c7>:1: DeprecationWarning: The module `kfp.v2` is deprecated and will be removed in a futureversion. Please import directly from the `kfp` namespace, instead of `kfp.v2`.\n",
            "  from kfp.v2.dsl import component, InputPath, Input, Output, Dataset\n"
          ]
        }
      ],
      "source": [
        "from kfp.v2.dsl import component, InputPath, Input, Output, Dataset\n",
        "\n",
        "@component(base_image='python:3.11',\n",
        "           packages_to_install=[\"pandas\"])\n",
        "def feature_selection(\n",
        "    X_train_input: Input[Dataset],\n",
        "    X_train_output: Output[Dataset],\n",
        "    X_val_input: Input[Dataset],\n",
        "    X_val_output: Output[Dataset],\n",
        "    X_test_input: Input[Dataset],\n",
        "    X_test_output: Output[Dataset]\n",
        "\n",
        "):\n",
        "    import pandas as pd\n",
        "\n",
        "    # Use the .path property to read and write data\n",
        "    X_train = pd.read_csv(X_train_input.path)\n",
        "    X_val = pd.read_csv(X_val_input.path)\n",
        "\n",
        "    X_test = pd.read_csv(X_test_input.path)\n",
        "\n",
        "    # Select specific feature columns\n",
        "    X_train = X_train[['age', 'sysBP', 'prevalentHyp', 'glucose', 'BPMeds']]\n",
        "    X_val = X_val[['age', 'sysBP', 'prevalentHyp', 'glucose', 'BPMeds']]\n",
        "\n",
        "    X_test = X_test[['age', 'sysBP', 'prevalentHyp', 'glucose', 'BPMeds']]\n",
        "\n",
        "    # Save to the path provided by Dataset\n",
        "    X_train.to_csv(X_train_output.path, index=False)\n",
        "    X_val.to_csv(X_val_output.path, index=False)\n",
        "\n",
        "    X_test.to_csv(X_test_output.path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Nav2K3ZY_FeX",
      "metadata": {
        "id": "Nav2K3ZY_FeX"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FODxXQU5_AJZ",
      "metadata": {
        "id": "FODxXQU5_AJZ"
      },
      "source": [
        "### Impute 'BPMeds', 'glucose' in X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6b7FoWyn_C_h",
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1714619719545,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "6b7FoWyn_C_h"
      },
      "outputs": [],
      "source": [
        "from kfp.dsl import component, Input, Output, Dataset, Artifact\n",
        "\n",
        "@component(base_image='python:3.11',\n",
        "           packages_to_install=[\"pandas\"])\n",
        "def impute_missing_value_train(\n",
        "           X_train_input: Input[Dataset],\n",
        "           X_train_output: Output[Dataset],\n",
        "           mode_bpm_output: Output[Artifact],\n",
        "           median_glucose_output: Output[Artifact]\n",
        "):\n",
        "    import pandas as pd\n",
        "\n",
        "    # Load data from input paths\n",
        "    X_train = pd.read_csv(X_train_input.path)  # Use .path to access the dataset\n",
        "\n",
        "    # Calculate the mode of 'BPMeds' and the median of 'glucose'\n",
        "    mode_bpm = X_train['BPMeds'].mode()[0]\n",
        "    median_glucose = X_train['glucose'].median()\n",
        "\n",
        "    # Write mode and median to their respective output paths\n",
        "    with open(mode_bpm_output.path, 'w') as file:\n",
        "        file.write(str(mode_bpm))\n",
        "    with open(median_glucose_output.path, 'w') as file:\n",
        "        file.write(str(median_glucose))\n",
        "\n",
        "    # Impute missing values in the training dataset\n",
        "    X_train['BPMeds'] = X_train['BPMeds'].fillna(mode_bpm)\n",
        "    X_train['glucose'] = X_train['glucose'].fillna(median_glucose)\n",
        "\n",
        "    # Save the imputed training dataset\n",
        "    X_train.to_csv(X_train_output.path, index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8aeAboM15Ab",
      "metadata": {
        "id": "e8aeAboM15Ab"
      },
      "source": [
        "### Impute 'BPMeds', 'glucose' in X_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "1nkwHk3JOjxa",
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1714619719545,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "1nkwHk3JOjxa"
      },
      "outputs": [],
      "source": [
        "from kfp.dsl import component, Input, Output, Dataset, Artifact\n",
        "\n",
        "@component(base_image='python:3.11',\n",
        "           packages_to_install=[\"pandas\"])\n",
        "def impute_missing_value_val(\n",
        "           X_val_input: Input[Dataset],\n",
        "           X_val_output: Output[Dataset],\n",
        "           mode_bpm_info: Input[Artifact],\n",
        "           median_glucose_info: Input[Artifact]\n",
        "):\n",
        "    import pandas as pd\n",
        "\n",
        "    # Load validation dataset\n",
        "    X_val = pd.read_csv(X_val_input.path)\n",
        "\n",
        "    # Read mode and median values from files\n",
        "    with open(mode_bpm_info.path, 'r') as file:\n",
        "        mode_bpm = float(file.read())\n",
        "    with open(median_glucose_info.path, 'r') as file:\n",
        "        median_glucose = float(file.read())\n",
        "\n",
        "    # Impute missing values in the validation dataset\n",
        "    X_val['BPMeds'] = X_val['BPMeds'].fillna(mode_bpm)\n",
        "    X_val['glucose'] = X_val['glucose'].fillna(median_glucose)\n",
        "\n",
        "    # Save the imputed validation dataset\n",
        "    X_val.to_csv(X_val_output.path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-1Vu-f8VW1-9",
      "metadata": {
        "id": "-1Vu-f8VW1-9"
      },
      "source": [
        "### Impute 'BPMeds', 'glucose' in X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "Gk-nbf6NW4sb",
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1714619719545,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "Gk-nbf6NW4sb"
      },
      "outputs": [],
      "source": [
        "from kfp.dsl import component, Input, Output, Dataset, Artifact\n",
        "\n",
        "@component(base_image='python:3.11',\n",
        "           packages_to_install=[\"pandas\"])\n",
        "def impute_missing_value_test(\n",
        "           X_test_input: Input[Dataset],\n",
        "           X_test_output: Output[Dataset],\n",
        "           mode_bpm_info: Input[Artifact],\n",
        "           median_glucose_info: Input[Artifact]\n",
        "):\n",
        "    import pandas as pd\n",
        "\n",
        "    # Load validation dataset\n",
        "    X_test = pd.read_csv(X_test_input.path)\n",
        "\n",
        "    # Read mode and median values from files\n",
        "    with open(mode_bpm_info.path, 'r') as file:\n",
        "        mode_bpm = float(file.read())\n",
        "    with open(median_glucose_info.path, 'r') as file:\n",
        "        median_glucose = float(file.read())\n",
        "\n",
        "    # Impute missing values in the validation dataset\n",
        "    X_test['BPMeds'] = X_test['BPMeds'].fillna(mode_bpm)\n",
        "    X_test['glucose'] = X_test['glucose'].fillna(median_glucose)\n",
        "\n",
        "    # Save the imputed validation dataset\n",
        "    X_test.to_csv(X_test_output.path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DSRHthv_NLyL",
      "metadata": {
        "id": "DSRHthv_NLyL"
      },
      "source": [
        "### Data quality steps (dealing with outliers and skewness) in X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "MuZCM1zjNO3B",
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1714619719545,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "MuZCM1zjNO3B"
      },
      "outputs": [],
      "source": [
        "@component(base_image='python:3.11',\n",
        "           packages_to_install=[\"pandas\", \"numpy\"])\n",
        "def data_quality_train(\n",
        "    X_train_input: Input[Dataset],\n",
        "    X_train_output: Output[Dataset],\n",
        "    IQR_train_output: Output[Artifact]\n",
        "):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    X_train = pd.read_csv(X_train_input.path)\n",
        "    def log_transform(x):\n",
        "        return np.log1p(x)\n",
        "\n",
        "    continuous_features = ['age', 'sysBP', 'glucose']\n",
        "    X_train[continuous_features] = X_train[continuous_features].apply(log_transform)\n",
        "\n",
        "    Q1 = X_train[continuous_features].quantile(0.25)\n",
        "    Q3 = X_train[continuous_features].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Explicitly set the index for bounds DataFrame to match continuous_features\n",
        "    bounds = pd.DataFrame({\n",
        "        'lower_bound': Q1 - 1.5 * IQR,\n",
        "        'upper_bound': Q3 + 1.5 * IQR\n",
        "    }, index=continuous_features)\n",
        "\n",
        "    # Save bounds to CSV ensuring index is included\n",
        "    bounds.to_csv(IQR_train_output.path)\n",
        "\n",
        "    for feature in continuous_features:\n",
        "        lower_bound = bounds.loc[feature, 'lower_bound']\n",
        "        upper_bound = bounds.loc[feature, 'upper_bound']\n",
        "        X_train[feature] = np.where(X_train[feature] < lower_bound, lower_bound, X_train[feature])\n",
        "        X_train[feature] = np.where(X_train[feature] > upper_bound, upper_bound, X_train[feature])\n",
        "\n",
        "    X_train.to_csv(X_train_output.path, index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zY6-jz_w1_Gf",
      "metadata": {
        "id": "zY6-jz_w1_Gf"
      },
      "source": [
        "### Data quality steps (dealing with outliers and skewness) in X_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "AEHd3l6b12TA",
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1714619719545,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "AEHd3l6b12TA"
      },
      "outputs": [],
      "source": [
        "@component(base_image='python:3.11',\n",
        "           packages_to_install=[\"pandas\", \"numpy\"])\n",
        "def data_quality_val(\n",
        "    X_val_input: Input[Dataset],\n",
        "    IQR_train_input: Input[Artifact],\n",
        "    X_val_output: Output[Dataset]\n",
        "):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    X_val = pd.read_csv(X_val_input.path)\n",
        "    IQR_stats = pd.read_csv(IQR_train_input.path, index_col=0)\n",
        "\n",
        "    # Check the index after loading\n",
        "    print(\"Indices in IQR_stats:\", IQR_stats.index)\n",
        "\n",
        "    def log_transform(x):\n",
        "        return np.log1p(x)\n",
        "\n",
        "    continuous_features = ['age', 'sysBP', 'glucose']\n",
        "    X_val[continuous_features] = X_val[continuous_features].apply(log_transform)\n",
        "\n",
        "    for feature in continuous_features:\n",
        "        lower_bound = IQR_stats.at[feature, 'lower_bound']\n",
        "        upper_bound = IQR_stats.at[feature, 'upper_bound']\n",
        "        X_val[feature] = np.where(X_val[feature] < lower_bound, lower_bound, X_val[feature])\n",
        "        X_val[feature] = np.where(X_val[feature] > upper_bound, upper_bound, X_val[feature])\n",
        "\n",
        "    X_val.to_csv(X_val_output.path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oM_Hs-2AXhsP",
      "metadata": {
        "id": "oM_Hs-2AXhsP"
      },
      "source": [
        "### Data quality steps (dealing with outliers and skewness) in X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "XNUZyOVuXjbT",
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1714619719545,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "XNUZyOVuXjbT"
      },
      "outputs": [],
      "source": [
        "@component(base_image='python:3.11',\n",
        "           packages_to_install=[\"pandas\", \"numpy\"])\n",
        "def data_quality_test(\n",
        "    X_test_input: Input[Dataset],\n",
        "    IQR_train_input: Input[Artifact],\n",
        "    X_test_output: Output[Dataset]\n",
        "):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    X_test = pd.read_csv(X_test_input.path)\n",
        "    IQR_stats = pd.read_csv(IQR_train_input.path, index_col=0)\n",
        "\n",
        "    # Check the index after loading\n",
        "    print(\"Indices in IQR_stats:\", IQR_stats.index)\n",
        "\n",
        "    def log_transform(x):\n",
        "        return np.log1p(x)\n",
        "\n",
        "    continuous_features = ['age', 'sysBP', 'glucose']\n",
        "    X_test[continuous_features] = X_test[continuous_features].apply(log_transform)\n",
        "\n",
        "    for feature in continuous_features:\n",
        "        lower_bound = IQR_stats.at[feature, 'lower_bound']\n",
        "        upper_bound = IQR_stats.at[feature, 'upper_bound']\n",
        "        X_test[feature] = np.where(X_test[feature] < lower_bound, lower_bound, X_test[feature])\n",
        "        X_test[feature] = np.where(X_test[feature] > upper_bound, upper_bound, X_test[feature])\n",
        "\n",
        "    X_test.to_csv(X_test_output.path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Qw8nwATZIbiv",
      "metadata": {
        "id": "Qw8nwATZIbiv"
      },
      "source": [
        "## Feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Gn5qLAB5IgMg",
      "metadata": {
        "id": "Gn5qLAB5IgMg"
      },
      "source": [
        "### Standardization X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "D_JJiBv_IyqP",
      "metadata": {
        "executionInfo": {
          "elapsed": 10,
          "status": "ok",
          "timestamp": 1714619719737,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "D_JJiBv_IyqP"
      },
      "outputs": [],
      "source": [
        "@component(base_image='python:3.11',\n",
        "           packages_to_install=[\"pandas\", \"scikit-learn\", \"joblib\"])  # Added joblib for model saving\n",
        "def standardize_train(\n",
        "    X_train_input: Input[Dataset],\n",
        "    X_train_output: Output[Dataset],\n",
        "    scaler_output_info: Output[Artifact]\n",
        "):\n",
        "    import pandas as pd\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    import joblib  # Import joblib for saving the scaler\n",
        "\n",
        "    X_train = pd.read_csv(X_train_input.path)\n",
        "    scaler = StandardScaler()\n",
        "    continuous_features = ['age', 'sysBP', 'glucose']\n",
        "\n",
        "    # Fit the scaler and transform the data\n",
        "    X_train_scaled = scaler.fit_transform(X_train[continuous_features])\n",
        "\n",
        "    # Convert the scaled array back into a DataFrame\n",
        "    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=continuous_features)\n",
        "\n",
        "    # Combine with non-scaled columns if needed\n",
        "    X_train[continuous_features] = X_train_scaled_df\n",
        "\n",
        "    # Save the DataFrame to CSV\n",
        "    X_train.to_csv(X_train_output.path, index=False)\n",
        "\n",
        "    # Save the scaler model to a file\n",
        "    joblib.dump(scaler, scaler_output_info.path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fB8zUR0-IwXk",
      "metadata": {
        "id": "fB8zUR0-IwXk"
      },
      "source": [
        "### Standardization X_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "fYqSYU9JKofZ",
      "metadata": {
        "executionInfo": {
          "elapsed": 10,
          "status": "ok",
          "timestamp": 1714619719737,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "fYqSYU9JKofZ"
      },
      "outputs": [],
      "source": [
        "@component(base_image='python:3.11',\n",
        "           packages_to_install=[\"pandas\", \"scikit-learn\", \"joblib\"])  # Added joblib for model loading\n",
        "def standardize_val(\n",
        "    X_val_input: Input[Dataset],\n",
        "    X_val_output: Output[Dataset],\n",
        "    scaler_input_info: Input[Artifact]\n",
        "):\n",
        "    import pandas as pd\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    import joblib  # Import joblib for loading the scaler\n",
        "\n",
        "    X_val = pd.read_csv(X_val_input.path)\n",
        "\n",
        "    # Load the scaler model from file\n",
        "    scaler = joblib.load(scaler_input_info.path)\n",
        "    continuous_features = ['age', 'sysBP', 'glucose']\n",
        "\n",
        "    # Transform the validation data\n",
        "    X_val_scaled = scaler.transform(X_val[continuous_features])\n",
        "\n",
        "    # Convert the scaled array back into a DataFrame\n",
        "    X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=continuous_features)\n",
        "\n",
        "    # Combine with non-scaled columns if needed\n",
        "    X_val[continuous_features] = X_val_scaled_df\n",
        "\n",
        "    # Save the DataFrame to CSV\n",
        "    X_val.to_csv(X_val_output.path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TvEzlNXXYH1D",
      "metadata": {
        "id": "TvEzlNXXYH1D"
      },
      "source": [
        "### Standardization X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "n44aDsloYJIL",
      "metadata": {
        "executionInfo": {
          "elapsed": 10,
          "status": "ok",
          "timestamp": 1714619719737,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "n44aDsloYJIL"
      },
      "outputs": [],
      "source": [
        "@component(base_image='python:3.11',\n",
        "           packages_to_install=[\"pandas\", \"scikit-learn\", \"joblib\"])  # Added joblib for model loading\n",
        "def standardize_test(\n",
        "    X_test_input: Input[Dataset],\n",
        "    X_test_output: Output[Dataset],\n",
        "    scaler_input_info: Input[Artifact]\n",
        "):\n",
        "    import pandas as pd\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    import joblib  # Import joblib for loading the scaler\n",
        "\n",
        "    X_test = pd.read_csv(X_test_input.path)\n",
        "\n",
        "    # Load the scaler model from file\n",
        "    scaler = joblib.load(scaler_input_info.path)\n",
        "    continuous_features = ['age', 'sysBP', 'glucose']\n",
        "\n",
        "    # Transform the validation data\n",
        "    X_test_scaled = scaler.transform(X_test[continuous_features])\n",
        "\n",
        "    # Convert the scaled array back into a DataFrame\n",
        "    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=continuous_features)\n",
        "\n",
        "    # Combine with non-scaled columns if needed\n",
        "    X_test[continuous_features] = X_test_scaled_df\n",
        "\n",
        "    # Save the DataFrame to CSV\n",
        "    X_test.to_csv(X_test_output.path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RfQeu3ktMUFo",
      "metadata": {
        "id": "RfQeu3ktMUFo"
      },
      "source": [
        "## Apply oversampling method (SMOTE) only on training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "4ib-tsRCManp",
      "metadata": {
        "executionInfo": {
          "elapsed": 10,
          "status": "ok",
          "timestamp": 1714619719737,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "4ib-tsRCManp"
      },
      "outputs": [],
      "source": [
        "@component(base_image='python:3.11',\n",
        "           packages_to_install=[\"pandas\", \"scikit-learn\",\"imbalanced-learn\"])  # Correct package name for imbalanced-learn\n",
        "def oversampling(\n",
        "    X_train_input: Input[Dataset],\n",
        "    y_train_input: Input[Dataset],  # Adding y_train as an input\n",
        "    X_train_output: Output[Dataset],\n",
        "    y_train_output: Output[Dataset],  # Adding an output for the resampled y_train\n",
        "    random_state: int = 42\n",
        "):\n",
        "    import pandas as pd\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "\n",
        "    # Load input data\n",
        "    X_train = pd.read_csv(X_train_input.path)\n",
        "    y_train = pd.read_csv(y_train_input.path)  # Assuming y_train is also a CSV file\n",
        "\n",
        "    # Create a SMOTE instance with the provided random state\n",
        "    sm = SMOTE(random_state=random_state)\n",
        "\n",
        "    # Apply SMOTE resampling\n",
        "    X_train_SMOTE, y_train_SMOTE = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Save the resampled data\n",
        "    X_train_SMOTE.to_csv(X_train_output.path, index=False)\n",
        "    y_train_SMOTE.to_csv(y_train_output.path, index=False)  # Save the resampled target variable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BOfMsSPDP5HM",
      "metadata": {
        "id": "BOfMsSPDP5HM"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KlTqXSMpP7S-",
      "metadata": {
        "id": "KlTqXSMpP7S-"
      },
      "source": [
        "## Logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RzbBHoY2T_nM",
      "metadata": {
        "id": "RzbBHoY2T_nM"
      },
      "source": [
        "### Train lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "UiTGl2CMP51x",
      "metadata": {
        "executionInfo": {
          "elapsed": 10,
          "status": "ok",
          "timestamp": 1714619719737,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "UiTGl2CMP51x"
      },
      "outputs": [],
      "source": [
        "from kfp.dsl import Metrics\n",
        "from kfp.dsl import Model\n",
        "\n",
        "@component(base_image='python:3.11', packages_to_install=[\"pandas\", \"scikit-learn\", \"joblib\"])\n",
        "def train_logistic_regression(\n",
        "    X_train_input: Input[Dataset],\n",
        "    y_train_input: Input[Dataset],\n",
        "    logistic_model_output: Output[Model]  # Corrected the name here for clarity\n",
        "):\n",
        "\n",
        "    import joblib\n",
        "    import pandas as pd\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    import os\n",
        "\n",
        "\n",
        "    # Load the training data\n",
        "    X_train_SMOTE = pd.read_csv(X_train_input.path)\n",
        "    y_train_SMOTE = pd.read_csv(y_train_input.path)\n",
        "\n",
        "    # Create a logistic regression model\n",
        "    logistic_model = LogisticRegression()\n",
        "\n",
        "    logistic_model.fit(X_train_SMOTE, y_train_SMOTE)\n",
        "\n",
        "\n",
        "    # Save the model to the designated gcs output path\n",
        "    os.makedirs(logistic_model_output.path, exist_ok=True)\n",
        "    joblib.dump(logistic_model, os.path.join(logistic_model_output.path, \"model.joblib\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QpxIlEUDUDR5",
      "metadata": {
        "id": "QpxIlEUDUDR5"
      },
      "source": [
        "### Evaluate lr on val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "vqFOloQmU6LR",
      "metadata": {
        "executionInfo": {
          "elapsed": 10,
          "status": "ok",
          "timestamp": 1714619719737,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "vqFOloQmU6LR"
      },
      "outputs": [],
      "source": [
        "from kfp.dsl import Metrics\n",
        "\n",
        "@component(base_image='python:3.11',\n",
        "           packages_to_install=[\"pandas\", \"scikit-learn\", \"joblib\"])\n",
        "def evaluate_logistic_regression(\n",
        "    X_val_input: Input[Dataset],\n",
        "    y_val_input: Input[Dataset],\n",
        "    logistic_model_input: Input[Model],\n",
        "    metrics: Output[Metrics]\n",
        "):\n",
        "\n",
        "    from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, precision_recall_curve, auc\n",
        "    import joblib\n",
        "    import pandas as pd\n",
        "\n",
        "    # Load the validation data\n",
        "    X_val_scaled = pd.read_csv(X_val_input.path)\n",
        "    y_val = pd.read_csv(y_val_input.path)\n",
        "\n",
        "\n",
        "\n",
        "    # Load the model\n",
        "    logistic_model_file_path = logistic_model_input.path + \"/model.joblib\"\n",
        "    logistic_trained_model = joblib.load(logistic_model_file_path)\n",
        "\n",
        "    # Make predictions on the validation data\n",
        "    y_val_pred = logistic_trained_model.predict(X_val_scaled)\n",
        "    y_val_probs = logistic_trained_model.predict_proba(X_val_scaled)[:, 1]  # probabilities for the positive class\n",
        "\n",
        "    # Calculate metrics\n",
        "    logistic_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    logistic_precision = precision_score(y_val, y_val_pred, average='weighted')\n",
        "    logistic_recall = recall_score(y_val, y_val_pred, average='weighted')\n",
        "    logistic_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
        "    logistic_auc = roc_auc_score(y_val, y_val_probs)\n",
        "\n",
        "    # Confusion matrix\n",
        "    logistic_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
        "\n",
        "    # Precision-recall curve and AUC-PR\n",
        "    precision, recall, _ = precision_recall_curve(y_val, y_val_probs)\n",
        "    logistic_auc_pr = auc(recall, precision)\n",
        "\n",
        "    # Log metrics\n",
        "    metrics.log_metric(\"Accuracy\", logistic_accuracy)\n",
        "    metrics.log_metric(\"Precision\", logistic_precision)\n",
        "    metrics.log_metric(\"Recall\", logistic_recall)\n",
        "    metrics.log_metric(\"F1 Score\", logistic_f1)\n",
        "    metrics.log_metric(\"AUC-ROC\", logistic_auc)\n",
        "    metrics.log_metric(\"AUC-PR\", logistic_auc_pr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5h05LRDhP-Qu",
      "metadata": {
        "id": "5h05LRDhP-Qu"
      },
      "source": [
        "## Random forest"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "013m_ROsrYo-",
      "metadata": {
        "id": "013m_ROsrYo-"
      },
      "source": [
        "### Train rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "wWzGYYe3rfqw",
      "metadata": {
        "executionInfo": {
          "elapsed": 10,
          "status": "ok",
          "timestamp": 1714619719737,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "wWzGYYe3rfqw"
      },
      "outputs": [],
      "source": [
        "@component(base_image='python:3.11', packages_to_install=[\"pandas\", \"scikit-learn\", \"joblib\"])\n",
        "def train_random_forest(\n",
        "    X_train_input: Input[Dataset],\n",
        "    y_train_input: Input[Dataset],\n",
        "    rf_model_output: Output[Model]  # Corrected the name here for clarity\n",
        "):\n",
        "\n",
        "    import joblib\n",
        "    import pandas as pd\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    import os\n",
        "\n",
        "    # Load the training data\n",
        "    X_train_SMOTE = pd.read_csv(X_train_input.path)\n",
        "    y_train_SMOTE = pd.read_csv(y_train_input.path)\n",
        "\n",
        "    # Create a random forest model\n",
        "    rf_model = RandomForestClassifier()\n",
        "\n",
        "    rf_model.fit(X_train_SMOTE, y_train_SMOTE)\n",
        "\n",
        "    # Save the model to gcs\n",
        "    os.makedirs(rf_model_output.path, exist_ok=True)\n",
        "    joblib.dump(rf_model, os.path.join(rf_model_output.path, \"model.joblib\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qVSw6LsFrbb4",
      "metadata": {
        "id": "qVSw6LsFrbb4"
      },
      "source": [
        "### Evaluate rf on val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "PQeboys-sJLI",
      "metadata": {
        "executionInfo": {
          "elapsed": 10,
          "status": "ok",
          "timestamp": 1714619719737,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "PQeboys-sJLI"
      },
      "outputs": [],
      "source": [
        "@component(base_image='python:3.11',\n",
        "           packages_to_install=[\"pandas\", \"scikit-learn\", \"joblib\"])\n",
        "def evaluate_random_forest(\n",
        "    X_val_input: Input[Dataset],\n",
        "    y_val_input: Input[Dataset],\n",
        "    rf_model_input: Input[Model],\n",
        "    metrics: Output[Metrics]\n",
        "):\n",
        "\n",
        "    from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, precision_recall_curve, auc\n",
        "    import joblib\n",
        "    import pandas as pd\n",
        "\n",
        "    # Load the validation data\n",
        "    X_val_scaled = pd.read_csv(X_val_input.path)\n",
        "    y_val = pd.read_csv(y_val_input.path)\n",
        "\n",
        "    # Load the model\n",
        "    rf_model_file_path = rf_model_input.path + \"/model.joblib\"\n",
        "    rf_trained_model = joblib.load(rf_model_file_path)\n",
        "\n",
        "    # Make predictions on the validation data\n",
        "    y_val_pred = rf_trained_model.predict(X_val_scaled)\n",
        "    y_val_probs = rf_trained_model.predict_proba(X_val_scaled)[:, 1]  # probabilities for the positive class\n",
        "\n",
        "    # Calculate metrics\n",
        "    rf_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    rf_precision = precision_score(y_val, y_val_pred, average='weighted')\n",
        "    rf_recall = recall_score(y_val, y_val_pred, average='weighted')\n",
        "    rf_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
        "    rf_auc = roc_auc_score(y_val, y_val_probs)\n",
        "\n",
        "    # Confusion matrix\n",
        "    rf_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
        "\n",
        "    # Precision-recall curve and AUC-PR\n",
        "    precision, recall, _ = precision_recall_curve(y_val, y_val_probs)\n",
        "    rf_auc_pr = auc(recall, precision)\n",
        "\n",
        "    # Log metrics\n",
        "    metrics.log_metric(\"Accuracy\", rf_accuracy)\n",
        "    metrics.log_metric(\"Precision\", rf_precision)\n",
        "    metrics.log_metric(\"Recall\", rf_recall)\n",
        "    metrics.log_metric(\"F1 Score\", rf_f1)\n",
        "    metrics.log_metric(\"AUC-ROC\", rf_auc)\n",
        "    metrics.log_metric(\"AUC-PR\", rf_auc_pr)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J37FOl58BJqp",
      "metadata": {
        "id": "J37FOl58BJqp"
      },
      "source": [
        "# Predict_and_combine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "91K0u31CBNzo",
      "metadata": {
        "executionInfo": {
          "elapsed": 10,
          "status": "ok",
          "timestamp": 1714619719737,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "91K0u31CBNzo"
      },
      "outputs": [],
      "source": [
        "@component(base_image='python:3.11',\n",
        "           packages_to_install=[\"pandas\", \"scikit-learn\", \"joblib\", \"gcsfs\"])\n",
        "def predict_and_combine(\n",
        "    test_features_path: InputPath('Dataset'),\n",
        "    patient_ids_path: InputPath('Dataset'),\n",
        "    model_path: str,\n",
        "    predictions_path: OutputPath('Dataset')\n",
        "):\n",
        "    import pandas as pd\n",
        "    import joblib\n",
        "    import gcsfs\n",
        "\n",
        "    # Create a GCS file system object\n",
        "    fs = gcsfs.GCSFileSystem()\n",
        "\n",
        "    # Load the trained model\n",
        "    with fs.open(model_path, 'rb') as f:\n",
        "      model = joblib.load(f)\n",
        "\n",
        "    # Load the test features\n",
        "    test_features = pd.read_csv(test_features_path)\n",
        "\n",
        "    # Load patient IDs\n",
        "    patient_ids = pd.read_csv(patient_ids_path)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(test_features)\n",
        "\n",
        "    # Combine patientID and predictions\n",
        "    results = pd.DataFrame({\n",
        "        'patientID': patient_ids['patientID'],\n",
        "        'TenYearCHD': predictions\n",
        "    })\n",
        "\n",
        "    # Save the results to the specified path\n",
        "    results.to_csv(predictions_path, index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A6YOhMKAkipe",
      "metadata": {
        "id": "A6YOhMKAkipe"
      },
      "source": [
        "# Define pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "6MSlXOAKkjPJ",
      "metadata": {
        "executionInfo": {
          "elapsed": 178,
          "status": "ok",
          "timestamp": 1714620093284,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "6MSlXOAKkjPJ"
      },
      "outputs": [],
      "source": [
        "@pipeline(name='CHD_pipeline')\n",
        "def CHD_pipeline():\n",
        "    # Import data\n",
        "    import_data_task = import_data()\n",
        "\n",
        "    # Import test data\n",
        "    import_test_data_task = import_test_data()\n",
        "\n",
        "    # Sperate ID\n",
        "    separate_id_task = separate_id(\n",
        "        df_input=import_data_task.outputs['df_output']\n",
        "\n",
        "    )\n",
        "\n",
        "    separate_id_test_task = separate_id_test(\n",
        "        df_test_input=import_test_data_task.outputs['df_test_output']\n",
        "    )\n",
        "\n",
        "    # drop_target_test_task = separate_id_test(\n",
        "    #     processed_test_data_input=separate_id_test_task.outputs['processed_test_data_output']\n",
        "    # )\n",
        "\n",
        "    # Split the dataset into training and validation sets\n",
        "    split_data_task = split_data(\n",
        "        processed_data_input = separate_id_task. outputs['processed_data_output']\n",
        "    )\n",
        "\n",
        "    # Feature selection\n",
        "    feature_selection_task = feature_selection(\n",
        "        X_train_input=split_data_task.outputs['X_train_output'],\n",
        "        X_val_input=split_data_task.outputs['X_val_output'],\n",
        "        X_test_input=separate_id_test_task.outputs['processed_test_data_output']\n",
        "    )\n",
        "\n",
        "    # Data preprocessing\n",
        "    impute_missing_value_train_task = impute_missing_value_train(\n",
        "        X_train_input=feature_selection_task.outputs['X_train_output']\n",
        "    )\n",
        "\n",
        "    impute_missing_value_val_task = impute_missing_value_val(\n",
        "        X_val_input=feature_selection_task.outputs['X_val_output'],\n",
        "        mode_bpm_info=impute_missing_value_train_task.outputs['mode_bpm_output'],\n",
        "        median_glucose_info=impute_missing_value_train_task.outputs['median_glucose_output']\n",
        "    )\n",
        "\n",
        "    impute_missing_value_test_task = impute_missing_value_test(\n",
        "        X_test_input=feature_selection_task.outputs['X_test_output'],\n",
        "        mode_bpm_info=impute_missing_value_train_task.outputs['mode_bpm_output'],\n",
        "        median_glucose_info=impute_missing_value_train_task.outputs['median_glucose_output']\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    data_quality_train_task = data_quality_train(\n",
        "        X_train_input=impute_missing_value_train_task.outputs['X_train_output']\n",
        "    )\n",
        "\n",
        "    data_quality_val_task = data_quality_val(\n",
        "        X_val_input=impute_missing_value_val_task.outputs['X_val_output'],\n",
        "        IQR_train_input=data_quality_train_task.outputs['IQR_train_output']\n",
        "    )\n",
        "\n",
        "    data_quality_test_task = data_quality_test(\n",
        "        X_test_input=impute_missing_value_test_task.outputs['X_test_output'],\n",
        "        IQR_train_input=data_quality_train_task.outputs['IQR_train_output']\n",
        "    )\n",
        "\n",
        "\n",
        "    standardize_train_task = standardize_train(\n",
        "        X_train_input=data_quality_train_task.outputs['X_train_output']\n",
        "    )\n",
        "\n",
        "    standardize_val_task = standardize_val(\n",
        "        X_val_input=data_quality_val_task.outputs['X_val_output'],\n",
        "        scaler_input_info=standardize_train_task.outputs['scaler_output_info']\n",
        "    )\n",
        "\n",
        "    standardize_test_task = standardize_test(\n",
        "        X_test_input=data_quality_test_task.outputs['X_test_output'],\n",
        "        scaler_input_info=standardize_train_task.outputs['scaler_output_info']\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    oversampling_task = oversampling(\n",
        "        X_train_input=standardize_train_task.outputs['X_train_output'],\n",
        "        y_train_input=split_data_task.outputs['y_train_output']\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    train_logistic_regression_task = train_logistic_regression(\n",
        "        X_train_input=oversampling_task.outputs['X_train_output'],\n",
        "        y_train_input=oversampling_task.outputs['y_train_output']\n",
        "    )\n",
        "\n",
        "    train_random_forest_task = train_random_forest(\n",
        "        X_train_input=oversampling_task.outputs['X_train_output'],\n",
        "        y_train_input=oversampling_task.outputs['y_train_output']\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluate_logistic_regression_task = evaluate_logistic_regression(\n",
        "        X_val_input=standardize_val_task.outputs['X_val_output'],\n",
        "        y_val_input=split_data_task.outputs['y_val_output'],\n",
        "        logistic_model_input=train_logistic_regression_task.outputs['logistic_model_output']\n",
        "    )\n",
        "\n",
        "    evaluate_random_forest_task = evaluate_random_forest(\n",
        "        X_val_input=standardize_val_task.outputs['X_val_output'],\n",
        "        y_val_input=split_data_task.outputs['y_val_output'],\n",
        "        rf_model_input=train_random_forest_task.outputs['rf_model_output']\n",
        "    )\n",
        "\n",
        "    # Predict and combine\n",
        "    predict_and_combine_task = predict_and_combine(\n",
        "        test_features_path=standardize_test_task.outputs['X_test_output'],\n",
        "        patient_ids_path=separate_id_test_task.outputs['ID_test_output'],\n",
        "        model_path='gs://ise543_final_pj/125143825809/chd-pipeline-20240501211757/train-random-forest_3515890258916933632/rf_model_output/model.joblib'\n",
        "\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H6BP8XGDkXTm",
      "metadata": {
        "id": "H6BP8XGDkXTm"
      },
      "source": [
        "# Compile and run pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "YiNdD7xskZls",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 12140,
          "status": "ok",
          "timestamp": 1714621436172,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "YiNdD7xskZls",
        "outputId": "40c6b9fa-9b3a-46d4-e272-f80a7142aa4d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/125143825809/locations/us-central1/pipelineJobs/chd-pipeline-20240502034344\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/125143825809/locations/us-central1/pipelineJobs/chd-pipeline-20240502034344')\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
            "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/chd-pipeline-20240502034344?project=125143825809\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/125143825809/locations/us-central1/pipelineJobs/chd-pipeline-20240502034344 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/125143825809/locations/us-central1/pipelineJobs/chd-pipeline-20240502034344\n"
          ]
        }
      ],
      "source": [
        "from kfp import compiler\n",
        "\n",
        "compiler.Compiler().compile(\n",
        "    pipeline_func=CHD_pipeline,\n",
        "    package_path = 'CHD_pipeline.json'\n",
        ")\n",
        "\n",
        "pipeline_job = aiplatform.PipelineJob(\n",
        "    display_name='CHD_pipeline',\n",
        "    template_path='CHD_pipeline.json',\n",
        "    pipeline_root='gs://ise543_final_pj',\n",
        "\n",
        "    enable_caching=True\n",
        ")\n",
        "\n",
        "pipeline_job.run()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ise543_final_proj_pipeline",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
